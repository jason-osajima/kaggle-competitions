{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "import random\n",
    "from datetime import date, datetime, timedelta\n",
    "import gc\n",
    "from io import StringIO\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n",
    "         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n",
    "        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\n",
    "PRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }\n",
    "\n",
    "BUCKET = 'dtci-dataplatform-telemetry-datsci-dev-bucket'\n",
    "S3_PATH = 'Jason/m5'\n",
    "CLIENT = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_start = 1500\n",
    "dataset_end = 1913\n",
    "nrows = None\n",
    "\n",
    "numcols = [f\"d_{day}\" for day in range(dataset_start, dataset_end + 1)]\n",
    "#catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
    "catcols = ['id']\n",
    "dtype = {numcol:\"float32\" for numcol in numcols} \n",
    "dtype.update({col: \"category\" for col in catcols if col != \"id\"}) \n",
    "\n",
    "filename = 'sales_train_validation.csv'\n",
    "obj = CLIENT.get_object(Bucket = BUCKET, Key = os.path.join(S3_PATH, filename))\n",
    "df = pd.read_csv(obj['Body'], nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n",
    "\n",
    "filename = 'calendar.csv'\n",
    "obj = CLIENT.get_object(Bucket = BUCKET, Key = os.path.join(S3_PATH, filename))\n",
    "cal =  pd.read_csv(obj['Body'], dtype = CAL_DTYPES)\n",
    "cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n",
    "for col, col_dtype in CAL_DTYPES.items():\n",
    "    if col_dtype == \"category\":\n",
    "        cal[col] = cal[col].cat.codes.astype(\"int16\")\n",
    "        cal[col] -= cal[col].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timespan(df, end, periods, freq='D'):\n",
    "    \"\"\"\n",
    "    Returns subset of dataframe sliced by a given range of dates (start : start + periods)\n",
    "    \"\"\"\n",
    "    return df[pd.date_range(end=end, periods=periods, freq=freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index to id\n",
    "df = df.set_index('id')\n",
    "# Map codes to actual dates\n",
    "d_to_date = dict(zip(cal['d'], cal['date']))\n",
    "date_to_d = dict(zip(cal['date'], cal['d']))\n",
    "df = df.rename(columns=d_to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_start = [1858, 1851, 1844, 1837, 1830, 1823, 1816]\n",
    "valid_y_start = 1886\n",
    "test_y_start = 1914\n",
    "\n",
    "# convert to datetime\n",
    "train_y_start = [d_to_date['d_{}'.format(integer_date)] for integer_date in train_y_start]\n",
    "valid_y_start = d_to_date['d_{}'.format(valid_y_start)]\n",
    "test_y_start = d_to_date['d_{}'.format(test_y_start)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, y_start, is_train = True):\n",
    "    # the X_start is 1 day before y_start\n",
    "    X_start = y_start - pd.Timedelta(days=1)\n",
    "    \n",
    "    X = {}\n",
    "    \n",
    "    for i in [1, 3, 7, 14, 30, 60, 140]:\n",
    "        # aggregate total\n",
    "        X['agg_sales_{}_days'.format(i)] = get_timespan(df, X_start, i).sum(axis=1)\n",
    "        \n",
    "        # diff mean, exp decay, mean, median, min, max, std\n",
    "        tmp = get_timespan(df, X_start, i)\n",
    "        X['diff_mean_sales_{}_days'.format(i)] = tmp.diff(axis=1).mean(axis=1).values\n",
    "        X['exp_decay_sales_{}_days'.format(i)] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n",
    "        X['mean_sales_{}_days'.format(i)] = tmp.mean(axis=1).values\n",
    "        X['median_{}_days'.format(i)] = tmp.median(axis=1).values\n",
    "        X['min_{}_days'.format(i)] = tmp.min(axis=1).values\n",
    "        X['max_{}_days'.format(i)] = tmp.max(axis=1).values\n",
    "        X['std_{}_days'.format(i)] = tmp.std(axis=1).values\n",
    "        \n",
    "        # diff mean, exp decay, mean, median, min, max, std lagged by 1 week\n",
    "        tmp = get_timespan(df, X_start - pd.Timedelta(days = 7), i)\n",
    "        X['diff_mean_sales_1weeklag_{}_days'.format(i)] = tmp.diff(axis=1).mean(axis=1).values\n",
    "        X['exp_decay_sales_1weeklag_{}_days'.format(i)] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n",
    "        X['mean_sales_1weeklag_{}_days'.format(i)] = tmp.mean(axis=1).values\n",
    "        X['median_1weeklag_{}_days'.format(i)] = tmp.median(axis=1).values\n",
    "        X['min_1weeklag_{}_days'.format(i)] = tmp.min(axis=1).values\n",
    "        X['max_1weeklag_{}_days'.format(i)] = tmp.max(axis=1).values\n",
    "        X['std_1weeklag_{}_days'.format(i)] = tmp.std(axis=1).values\n",
    "        \n",
    "        # mean + exponential decay number of sales for days when there was at least one sale\n",
    "        tmp1 = get_timespan(df, X_start, i)\n",
    "        tmp2 = (get_timespan(df, X_start, i) > 0) * 1\n",
    "        X['mean_nonzero_sales_{}_days'.format(i)] = (tmp1 * tmp2.replace(0, np.nan)).mean(axis=1).values\n",
    "        X['exp_decay_nonzero_sales_{}_days'.format(i)] = (tmp1 * tmp2.replace(0, np.nan) * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n",
    "        \n",
    "        # mean + exponential decay number of days when there wasn't a sale\n",
    "        X['mean_number_of_days_with_no_sales_{}_days'.format(i)] = (1 - tmp2).mean(axis=1).values\n",
    "        X['exp_decay_number_of_days_with_no_sales_{}_days'.format(i)] = ((1 - tmp2) * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n",
    "        \n",
    "        # last sale day\n",
    "        tmp = get_timespan(df, X_start, i)\n",
    "        X['has_sale_in_past_{}_days'.format(i)] = (tmp > 0).sum(axis=1).values\n",
    "        X['days_since_last_sale_{}_days'] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values # if no sale in the past i days, return i\n",
    "        X['weighted_days_since_last_sale_{}_days'] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values\n",
    "    \n",
    "    for i in range(1, 16):\n",
    "        # actual sales\n",
    "        X['actual_number_of_sales_{}_days'.format(i)] = get_timespan(df, X_start - pd.Timedelta(days=i), 1).values.ravel()\n",
    "    \n",
    "    for i in range(7):\n",
    "        X['mean_DOW_{}_past_4_weeks'.format(i)] = get_timespan(df, X_start - pd.Timedelta(days=28-i), 4, freq='7D').mean(axis=1).values\n",
    "        X['mean_DOW_{}_past_20_weeks'.format(i)] = get_timespan(df, X_start - pd.Timedelta(days=140-i), 20, freq='7D').mean(axis=1).values\n",
    "    \n",
    "    X = pd.DataFrame(X)\n",
    "    \n",
    "    if is_train:\n",
    "        y = df[\n",
    "            pd.date_range(y_start, periods=28)\n",
    "        ].values\n",
    "        return X, y\n",
    "    \n",
    "    else:\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1, y_1 = [], []\n",
    "for y_start in train_y_start:\n",
    "    X_tmp, y_tmp = prepare_dataset(df, y_start)\n",
    "    X_1.append(X_tmp)\n",
    "    y_1.append(y_tmp)\n",
    "\n",
    "X_train = pd.concat(X_1, axis=0)\n",
    "y_train = np.concatenate(y_1, axis=0)\n",
    "\n",
    "del X_1, y_1; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = prepare_dataset(df, valid_y_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = prepare_dataset(df, test_y_start, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "Train one model for each day in y (28 models total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_leaves': 80,\n",
    "    'objective': 'poisson',\n",
    "    'min_data_in_leaf': 200,\n",
    "    'learning_rate': 0.02,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'metric': 'rmse',\n",
    "    'num_threads': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROUNDS = 10000\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "\n",
    "# placeholder for categorical variables\n",
    "cate_vars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 1\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[189]\ttraining's rmse: 1.95727\tvalid_1's rmse: 1.76406\n",
      "==================================================\n",
      "Step 2\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[249]\ttraining's rmse: 1.73667\tvalid_1's rmse: 1.85046\n",
      "==================================================\n",
      "Step 3\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[202]\ttraining's rmse: 1.85803\tvalid_1's rmse: 1.6936\n",
      "==================================================\n",
      "Step 4\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[213]\ttraining's rmse: 1.86609\tvalid_1's rmse: 1.81295\n",
      "==================================================\n",
      "Step 5\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[538]\ttraining's rmse: 1.78188\tvalid_1's rmse: 2.07575\n",
      "==================================================\n",
      "Step 6\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[499]\ttraining's rmse: 2.15472\tvalid_1's rmse: 2.30095\n",
      "==================================================\n",
      "Step 7\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[526]\ttraining's rmse: 2.14188\tvalid_1's rmse: 2.47391\n",
      "==================================================\n",
      "Step 8\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[391]\ttraining's rmse: 1.92154\tvalid_1's rmse: 2.04537\n",
      "==================================================\n",
      "Step 9\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[653]\ttraining's rmse: 1.69634\tvalid_1's rmse: 2.04424\n",
      "==================================================\n",
      "Step 10\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[961]\ttraining's rmse: 1.67325\tvalid_1's rmse: 2.10102\n",
      "==================================================\n",
      "Step 11\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[413]\ttraining's rmse: 1.83236\tvalid_1's rmse: 2.0146\n",
      "==================================================\n",
      "Step 12\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[606]\ttraining's rmse: 1.83209\tvalid_1's rmse: 2.14583\n",
      "==================================================\n",
      "Step 13\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[637]\ttraining's rmse: 2.16692\tvalid_1's rmse: 2.53768\n",
      "==================================================\n",
      "Step 14\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[679]\ttraining's rmse: 2.15393\tvalid_1's rmse: 2.43377\n",
      "==================================================\n",
      "Step 15\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[559]\ttraining's rmse: 1.92019\tvalid_1's rmse: 2.25766\n",
      "==================================================\n",
      "Step 16\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[709]\ttraining's rmse: 1.77886\tvalid_1's rmse: 2.10844\n",
      "==================================================\n",
      "Step 17\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[498]\ttraining's rmse: 1.83664\tvalid_1's rmse: 1.78323\n",
      "==================================================\n",
      "Step 18\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[479]\ttraining's rmse: 1.853\tvalid_1's rmse: 2.05688\n",
      "==================================================\n",
      "Step 19\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[695]\ttraining's rmse: 1.84363\tvalid_1's rmse: 2.26245\n",
      "==================================================\n",
      "Step 20\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[563]\ttraining's rmse: 2.25272\tvalid_1's rmse: 2.20636\n",
      "==================================================\n",
      "Step 21\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[591]\ttraining's rmse: 2.229\tvalid_1's rmse: 2.44043\n",
      "==================================================\n",
      "Step 22\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[591]\ttraining's rmse: 1.86397\tvalid_1's rmse: 2.04772\n",
      "==================================================\n",
      "Step 23\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[533]\ttraining's rmse: 1.83824\tvalid_1's rmse: 1.84509\n",
      "==================================================\n",
      "Step 24\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[308]\ttraining's rmse: 1.87613\tvalid_1's rmse: 1.79936\n",
      "==================================================\n",
      "Step 25\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[355]\ttraining's rmse: 1.8898\tvalid_1's rmse: 1.76585\n",
      "==================================================\n",
      "Step 26\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[546]\ttraining's rmse: 1.8987\tvalid_1's rmse: 1.91152\n",
      "==================================================\n",
      "Step 27\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[496]\ttraining's rmse: 2.26206\tvalid_1's rmse: 2.25459\n",
      "==================================================\n",
      "Step 28\n",
      "==================================================\n",
      "Training until validation scores don't improve for 125 rounds\n",
      "Early stopping, best iteration is:\n",
      "[228]\ttraining's rmse: 2.42984\tvalid_1's rmse: 2.2022\n"
     ]
    }
   ],
   "source": [
    "for i in range(28):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Step %d\" % (i+1))\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Instantiate Dataset\n",
    "    dtrain = lgb.Dataset(X_train.values, label=y_train[:, i], categorical_feature=cate_vars)\n",
    "    dval = lgb.Dataset(X_val.values, label=y_val[:, i], reference=dtrain, categorical_feature=cate_vars)\n",
    "    \n",
    "    # Train Model\n",
    "    bst = lgb.train(params, dtrain, num_boost_round=MAX_ROUNDS, \n",
    "                    valid_sets=[dtrain, dval], early_stopping_rounds=125, verbose_eval=MAX_ROUNDS)\n",
    "    val_pred.append(bst.predict(\n",
    "        X_val, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "    test_pred.append(bst.predict(\n",
    "        X_test, num_iteration=bst.best_iteration or MAX_ROUNDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mse: 4.380480390569589\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation mse:\", mean_squared_error(\n",
    "    y_val, np.array(val_pred).transpose()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data=np.array(test_pred).transpose(),  index=X_test.index)\n",
    "submission.columns = ['F{}'.format(i + 1) for i in submission.columns.values]\n",
    "submission.reset_index(inplace=True)\n",
    "\n",
    "# dummy data for evaluation\n",
    "submission_eval = submission.copy()\n",
    "submission_eval[\"id\"] = submission_eval[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "submission = pd.concat([submission, submission_eval], axis=0, sort=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'B801D68F8A2B002D',\n",
       "  'HostId': 'xWu2Lgewkhibv+7YFIqYT7WfsjteRR2olPhQB4y1ZrG5sEkmt5wfeFu2MHt11KNV5aS15FybCGM=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'xWu2Lgewkhibv+7YFIqYT7WfsjteRR2olPhQB4y1ZrG5sEkmt5wfeFu2MHt11KNV5aS15FybCGM=',\n",
       "   'x-amz-request-id': 'B801D68F8A2B002D',\n",
       "   'date': 'Sat, 18 Apr 2020 20:07:58 GMT',\n",
       "   'etag': '\"c38ef18cc2f95761f8bf299d06bafbcf\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"c38ef18cc2f95761f8bf299d06bafbcf\"'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'submission_sequential_lgbm_{}.csv'.format(date.today().strftime('%m%d_%h'))\n",
    "csv_buffer = StringIO()\n",
    "submission.to_csv(csv_buffer, index=False)\n",
    "s3_object = boto3.resource('s3').Object(BUCKET, os.path.join(S3_PATH, 'submissions', filename))\n",
    "s3_object.put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
